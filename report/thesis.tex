%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Sample document ``thesis.tex''
%%

% Available documentclass options:
% - <all `report` document class options, e.g.: `a5paper`>
\documentclass[]{simple-thesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Some mods
%%
\usepackage{graphicx}
\usepackage{courier}
\usepackage{listings}
\usepackage[margin=1cm]{caption}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  numbers=left
}
\newcommand\fnurl[2]{%
  \href{#2}{#1}\footnote{\url{#2}}%
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis meta-information
%%

%% The title of the thesis:
\title{UnsocialVR: Faking active listening in social virtual environments}

%% The full name of the author (e.g.: James Smith):
\author{Tom Gurion}

%% Affiliation:
\affiliation{Media and Arts Technology\\Queen Mary University of London}
\affiliationlogo{CollegeShields/QMUL}

%% You can redefine the submission notice [optional]:
\submissionnotice{Advanced placement project final report\\Supervisor: Patrick Heleay\\Host: Inition}

%% PDF meta-info:
\hypersetup{pdfsubject={TODO subject},pdfkeywords={TODO keywords}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%
\abstract{%
  TODO abstract...
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
\acknowledgements{%
  TODO acknowledgments...

  Inition.
  Patrick Healey.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%
\begin{document}

\frontmatter{}  %% Title page, abstract, TOC, etc.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%
\chapter{Introduction}

In Infinite Jest, David Foster Wallace argues that "Good old traditional audio-only phone conversations allowed you to presume that the person on the other end was paying complete attention to you while also permitting you not to have to pay anything even close to complete attention to her." \citep{Wallace1996}.
He continues and claims that we are addicted to this illusion, and that's why video conferencing always feel so awkward - we need to pretend to listen all the time.
And if we think about it, even in face to face conversation we must always adhere to these social rules, and signal our complete attention when someone is talking to us.

In this project I experiment with virtual reality (VR) technologies to see if this illusion of faking active listening is transferable to other mediums, and if so, how.
In Unsocial VR participants share the same virtual environment, using VR headsets and controllers.
They can converse freely and move around, and if they want to start faking listening to the conversation and just wander around, or even talk with other participants while faking, they absolutely can!
The interface is very minimal, just hit a button on the controller to start faking active listening behaviours towards your current conversation, and release it when you want to stop faking.
Players even get an on screen notification when someone is speaking directly to them, so they can return to the conversation elegantly.

More generally...
VR in general, and Social VR as new platform to investigate behaviours.


\chapter{Literature review}

This project is based on multidisciplinary research.
It merges ideas from telepresence and mediated communication, that explore ways to represent non verbal cues in new communication technologies.
It depends on multiparty social interaction analysis, which take spatial cues into account in understanding who is talking with whom in a "cocktail party" situation.
It explores new ways to generate active listening signals (or "backchannel behaviours", as they called in the scientific literature), such as head nods, in automated agents.

TODO Reread Beilenson and add here.

\section{Telepresence and mediated communication}

Surveys \citep{Isaacs1994, Erickson2000}.

Social VR.

The GAZE systems \citep{Vertegaal1999, Vertegaal2003}.

\section{Multiparty social interaction}

Spatial characteristics of face to face communication \citep{Schegloff1998}.
F-formation theory \citep{Kendon1990} and automated analysis \citep{Setti2015}.

\section{Active listening}

Speakers, addressees, and side-participants \citep{Clark1982}.

Backchannels analysis and automated active listeners.
Research based on set of rules \citep{Ward2000, Nishimura2007, Gratch2007}.
Data driven approaches \citep{Morency2008, Huang2011, Kok2012}.

Most research in backchannel behaviours is concentrated on two party conversations.
In the case of three party conversation, the role of the side-participant was less investigated in terms of automated analysis and generation.
Excluding \citep{Matsusaka2003, Fujie2009}.

State of the art research often based on close datasets \citep{Kok2011}.
Publicly available datasets \citep{Aubrey2013, Oertel2013}.
In the current research I use the ICT Rapport dataset \citep{Gratch2007} to train a backchannel model.

\section{Asymmetric communication}

\citep{Chou2016}, TODO cite Pat recommendation.


\chapter{System design and implementation}

TODO Requirements / targets.

TODO General description.

The code is available TODO after open sourcing.

Figure \ref{fig:system:diagram} shows a schematic diagram of the system, and the paths and protocols of communication between the components.
The components in blue represent client side code, those in green are the client facing servers, and the red ones are background servers that process behavioural data to instruct the client facing servers how to operate.
A thorough description of each component in the system is presented bellow.

\begin{figure}
  \includegraphics[width=\textwidth]{../graphics/system_diagram.pdf}
  \caption{A schematic diagram of the system architecture. In blue are client side components, that communicate with the client facing servers in green. The components in red are background servers that are used by the client facing servers to process behavioural data. The lines between the components represents paths of communication with the a label indicating the protocol.}
  \label{fig:system:diagram}
\end{figure}

\section{Hardware}

The hardware used in this project is the \fnurl{HTC Vive}{https://www.vive.com/uk/}, which bundle together a headset and two hand controllers.
In addition, \fnurl{Vive trackers}{https://www.vive.com/uk/vive-tracker/} are used to track the chest position of each participant.
This setup provided suitable features for the project compared to products like the \fnurl{Oculus Rift}{https://www.oculus.com/rift/}, or simpler VR solutions like \fnurl{Google Daydream}{https://vr.google.com/daydream/} or \fnurl{Samsung Gear VR}{http://www.samsung.com/global/galaxy/gear-vr/}.
Specifically, my system make use of the following features of the headset, that I couldn't find in other products:

\begin{itemize}
  \item The headset and its controllers and trackers are tracked in 3 dimensional (3D) space of up to 6 x 6 meters, enough for freely wondering around in a virtual environment. Recently, the Oculus Rift started to provide similar capabilities but they are better supported by the HTC Vive.
  \item The ability to combine extra trackers, that are used in this project to track the chest of the participant, are not available in other products.
\end{itemize}

\section{Game Client}

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{../graphics/environment_demo.png}
  \caption{Example view of the virtual environment, showing the cocktail party context and the avatar design.}
  \label{fig:system:environment_demo}
\end{figure}

The Game Client is the application each participant use to connect to the 3D virtual environment.
Figure \ref{fig:system:environment_demo} shows how the virtual environment and the avatars look like.
I choose to contextualize the environment as a cocktail party on the beach.
This decision is mainly influenced by the intent to create a shared space where people can interact and dynamically form conversational groups.
Note that cocktail parties are common examples for such environments in the scientific literature \citep{Setti2015}.

TODO uncanny valley and avatar design.

Most of the 3D models were designed by me using \fnurl{blender}{https://www.blender.org/}.
I choose blender mainly because it is an open source project, whereas most of the alternatives are non-free commercial software.
Choosing a free and open source solution will allow me to reuse the acquired skills in future projects.

The players interface is minimal.
There is one button on the controller that when pressed, the player starts to fake active listening towards the other players in the same F-formation.
When the button is released the faked behaviour stops and the player jumps back to the place and orientation where the faking behaviour started.
This gives the players the freedom to start faking active listening and jump back to the conversation as fast as possible when needed.
As in other HTC Vive games, one of the controller's buttons implements a ``teleportation'' feature that make movement in the environment faster and easier (like in \fnurl{The Lab}{http://store.steampowered.com/app/450390/The_Lab/} game).
In addition, while a player is faking active listening is approached by another player, a notification saying that ``someone is talking to you'' appears, suggesting that it's time to stop faking and going back to the conversation.

The client is developed using the free game development platform \fnurl{Unity3D}{https://unity3d.com/}.
The integration of the hardware with Unity3D is seamless, and both has a large community of developers, so it was a natural choice to build the client with this technology.

The client application communicate with two other components over the network.
The first is the Identification Game Server.
Communication between the clients and this server is based on the native Unity3D networking library: UNet.
The second component that the client communicate with, over HTTP, is the Main Game Server.

\section{Simulator Client}

For testing and development purposes I wrote a simulator client.
The simulator client uses the same environment and avatars.
But here, players use the keyboard to control their avatar and see the environment on the computer screen instead of the VR headset and controllers.
Figure \ref{fig:system:simulator_f_formation} shows how the environment and avatars look like in the simulator.

\section{Identification Game Server}

The Identification Game Server utilize the built-in Unity3D network server to create an identification for each connected client, and notify the rest of the clients about new connections.
Usually, the Unity3D network server is also used to pass positioning, orientation, and other kinds of information between clients.
In this project, however, all of the data is passed between the clients through the Main Game Server.

\section{Main Game Server}

The main game server is used to pass information between the clients.
Every Game Client send information about itself to the server every 20 milliseconds.
This fast communication is crucial for keeping the visual rendering smooth.
The information each client sends includes the positioning and orientation of the chest, head, and hands of the player, as well as who the player is looking at, and whether or not the player is speaking (using sound level threshold).
The server, however, doesn't blindly pass this information to the other clients.
It uses sophisticated analysis to manipulate the information it presents to each client to achieve the intended social illusions.

\begin{figure}
  \begin{lstlisting}[language=Python]
    player_id, player_data = deserialize(request.data)

    store_in_cache(player_id, player_data)

    other_players = get_others_from_cache(player_id)
    my_f_formation = get_f_formation_from_cache(player_id)

    for other_player in other_players:
        if other_player in my_f_formation.fakers:
            other_player.fake_active_listening(my_f_formation.speaker)
        else if other_player not in my_f_formation.members:
            other_player.scale_down()
           
    return serialize(other_players)
  \end{lstlisting}
  \caption{Pseudo-code for the HTTP request/response cycle in the Main Game Server that handle new data from a client and reply with information about the rest of the clients.}
  \label{fig:system:main_server_pseudocode}
\end{figure}

The server exposes three HTTP endpoints for the clients to communicate with.
Figure \ref{fig:system:main_server_pseudocode} shows a pseudo-code of the main request/response cycle in the server that is used by the client to send their information and get back information about the other players.
First, the player data is extracted from the request data and stored in the cache (lines 1 and 3).
Then, information about the rest of the players and the player F-formation are extracted from the cache (lines 5 and 6).
The state of each of the other players is determined by a series of checks.
If the other player is currently faking active listening in the requester F-formation, his/her positioning and orientation are overridden by automatic generated behaviour (line 10).
This is a merge of a baseline behaviour and predicted head nods that are described thoroughly in section \ref{system:fake_behaviour_generator}.
Alternatively, if the other player is not a member of the requester F-formation his/her avatar is scaled down, helping in separating conversational group in the virtual environment (line 12).
Otherwise, the other player is generated as is to the requester.
Lastly, the list of other players is serialized and returned in the response object (line 14).

The next two endpoints available for the clients are for starting and stopping the faked active listening behaviour.
The clients call these endpoints when the faking button is triggered and released respectively.

In addition to the client facing endpoints there are two background processes running as part of the Main Game Server.
They periodically send and receive data from the two background servers.
Every second, one background process collect the chest position and orientation of all of the players, project them on the 2 dimensional plane of the floor, and send them to the Scene Analysis Server for F-formation analysis.
The results are stored in a the server cache.
The second background process repeatedly fetches all the faker players from the cache, find their speakers (if exist), and pass the speaker talking state (is speaking or silent) and gaze (is looking or not looking at the faker) to the Active Listening Server to predict backchannel behaviours.

The server is developed in \fnurl{Elixir}{https://elixir-lang.org/}.
Usually, when developing games in Unity3D the server is built with Unity3D as well.
The main reason I choose Elixir instead is my prior familiarity with it and understanding its capabilities and design.
Being able to develop the system as fast as possible was crucial in this project, and using Unity3D as a server would probably slow me down.
In addition, there are some specific benefits for using Elixir in this type of project.
First, the language is designed for concurrent and fault tolerant systems.
In the context of this project the concurrent model allowed me to develop a system with relatively fast response rates while longer processes are always happen in the background and cached.
The fault tolerance capabilities allowed me to ignore handling incomplete cached data that happens every restart or when clients connect and disconnect.
With other technologies such inconsistencies will affect all clients, and the general behaviour of the system.
But with elixir, inconsistent states cause a relatively local failure, affecting only one client at a time in my case.
Regardless of the failures, after a few seconds the background processes are guaranteed to fix any discrepancies in the cached data.
Second, Elixir is a high level language with terse syntax and have many high quality libraries available that support the fast development of this server.

\section{Scene Analysis Server}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../graphics/simulator_f_formation.png}
  \caption{F-formation analysis, based on the GCFF algorithm, for 5 Simulator Clients scenario.}
  \label{fig:system:simulator_f_formation}
\end{figure}

The Scene Analysis Server is used to analyze F-formations of the players in the virtual environment, based on their positioning and orientation.
Figure \ref{fig:system:simulator_f_formation} shows an example of F-formation analysis for 5 Simulator Clients scenario.

This server runs a modified version\footnote{Open source and available online at \url{https://github.com/nagasaki45/gcff}} of the open source Graph-Cuts for F-formation (GCFF) detection algorithm by \cite{Setti2015}.
It is written in \fnurl{Matlab}{https://www.mathworks.com/products/matlab.html} and is \fnurl{available online}{https://github.com/franzsetti/GCFF}.
Originally developed to analyze still images, the GCFF algorithm detects F-formations using only $(x, y)$ positioning and orientation information in 2 dimensional space.

To integrate it into the system I had to slightly modify the code.
First, as opposed to the original use case, I had to identify F-formations over time.
To illustrate this need, consider a player faking active listening towards an F-formation.
Now, if the other players from that F-formation walk away, the F-formation breaks and the faker shouldn't keep pretending to listen.
It is especially noticeable when other players approaches the place were the faker stands, it should be clear that a new F-formation is formed.
Without the ability to identify F-formations over time the faker won't know that an F-formation broke and that faking active listening is not needed any more.
My implementation of the F-formation tracking over time is based on movement of the F-formations centres.
After each time the GCFF algorithm is called, the new F-formations are compared to the ones that were computed in the previous call.
If the distance of a new F-formation from an old one is bellow an arbitrary threshold, it will be considered as the same.
Otherwise, it is a new F-formation.
Second, to be able to call the algorithm from the Main Game Server I wrote a minimal HTTP server in \fnurl{Python}{https://www.python.org/}, that uses \fnurl{Octave}{https://www.gnu.org/software/octave/} and \fnurl{oct2py}{https://github.com/blink1073/oct2py} to invoke the Matlab code.

\section{Active Listening Server}

The Active Listening Server is responsible for predicting backchannel behaviours based on speaker talking state (is speaking or silence) and gaze (is looking at listener or not).
Predicting listeners backchannel behaviours is usually done using either rule based systems, or more recently, data driven and machine learning (ML) approaches \citep{Morency2008}.
Due to the better performance of the machine learning predictions I followed the same line and with the absence of openly available backchannel predictor decided to develop one myself.

Training an ML model required a dataset of conversations with annotations for speaker talking state and gaze, and with listener backchannel annotations.
The ICT Rapport Dataset \citep{Gratch2007} contains 126 annotated interactions between a speaker and listener, and is \fnurl{openly available online}{http://rapport.ict.usc.edu/}, making it ideal for training a backchannel predictor.
Many of the interactions, however, were not properly annotated with the necessary data (files missing or blank columns).
After filtering out interactions with missing data I had 48 interactions with average length of 138 seconds, ranging from 41 to 248 seconds.

It is important to note that there is an expected difference in social behaviour between an addressee and a side-participant \citep{Clark1982}.
For the current system, the faked behaviour is usually the one of the side-participant, and not of the addressee.
Although the addressee can fake active listening, it is more common for side-participants to fake and wonder around, as they are not required to provide verbal response to the speaker.
However, there is much less literature about predicting side-participants active listening behaviours, and even less available resource to train such model.
With that in mind, I settled for predicting the addressee backchannels and applied them to both addressees and side-participants.

To prepare the data for training I re-sampled the annotations every 100 milliseconds.
Then, to create samples representing a moving time window I concatenate 30 samples (3 seconds) of the speaker talking state and gaze into one vector.
This vector, when fed into the ML model, should predict the listeners nodding of the last sample.
As a last step of preparation I split the dataset into 3 to 1 train and test portions.

I tried to train three different ML models.
First I tried to train a Linear Support Vector Machine Classifier (LinearSVC) model using the open source package Scikit-learn \citep{Pedregosa2011}.
Using the test portion of the dataset, this model presented high precision (percentage of correct predictions) was high.
However, but the recall (correct predictions out of correct predictions and misses) was very low.
In fact, the model learned never to almost never predict a backchannel.
Visually comparing the predictions to the dataset reveals that the it failed to model the data correctly.
Second, I attempted to train a Long Short-Term Memory (LSTM) deep neural network with the open source package Keras \citep{Chollet2015}.
The results were similar to these of the LinearSVC model.
Lastly, I trained a K Nearest Neighbors classifier with Scikit-learn.
This time, it seems that the model manage to capture nature of the data.
Using the test portion of the dataset the was lower than before (0.77), but the recall was higher (0.83).

\begin{figure}
  \includegraphics[width=\textwidth]{../graphics/backchannel_predictions.pdf}
  \caption{An example of one interaction between a speaker and a listener from the ICT Rapport dataset, with both actual and predicted listener backchannel behaviours. The lines indicate the state of the features and prediction with values of either 0 when the line is low, or 1 when the line is high.}
  \label{fig:system:backchannel_predictions}
\end{figure}

Figure \ref{fig:system:backchannel_predictions} shows an example of backchannels predictions for an interaction predicted from the dataset.


Choosing between the models didn't follow a properly nor systematic evaluation.
However, the K Nearest Neighbors classifier performed well enough on the test dataset.
It also provided fast predictions, as necessary by real-time system like this.

TODO de Kok threshold.

The model, with the decreasing threshold algorithm, are written in Python and are exposed to the Main Gaim Server as an HTTP server.
The code for this server is open sourced and \fnurl{available online}{https://github.com/nagasaki45/backchannel}.

\section{Fake Behaviour Generator}\label{system:fake_behaviour_generator}

TODO Fake behaviour baseline recorder.


\chapter{Evaluation}

What I want to check?

Can participants detect the fake, automated behaviour of an active listener?
Is the level of communication suffer from listeners dropping out of the conversation?

How air balloon task with varied number of participants.
All participants are informed that they need to search for ``tokens'' in the virtual environment, while autopiloting towards the main conversation.

How do I record when someone is caught faking? One option is to do it automatically when someone is approached while autopiloting.
How do I record the success rate of participants in finding the tokens?
How to assess the level of discussion?

\section{Pilot}

The transition from ``real'' to ``fake'' behaviour is too noticeable.

\section{Methods}

\section{Results}

\section{Discussion}


\chapter{General discussion}


\chapter{Conclusions}

Use VR to investigate new ways of communication, and new social behaviours that are impossible in face to face conversation.

\section{Future work}

Many possible improvements to the idea and implementation.
Faking social behaviour is only one possibility.
Auto swap positioning.

\begin{itemize}
  \item Require attention based on end-of-turn analysis instead of when someone is speaking with me.
  \item Jumping seamlessly between autopilots.
  \item Playback addressee in 4 seconds delay a la \cite{Bailenson2005}.
  \item Train an ML algorithm on VR data, not to predict head nods, but to generate hands and head transforms directly.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography:
%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plainnat}
\bibliography{thesis}


\end{document}
